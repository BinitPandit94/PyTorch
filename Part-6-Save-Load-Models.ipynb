{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part-6-Save-Load-Models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX8N0u1jv9tq"
      },
      "outputs": [],
      "source": [
        "## Importing Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Model\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self,in_channels, num_classes):\n",
        "    super(CNN,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=(3,3), padding=(1,1)) # Same Convolution\n",
        "    self.pool = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
        "    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), padding=(1,1)) # Same Convolution\n",
        "    self.fc1 = nn.Linear(16*7*7,num_classes) # Will use 2 maxpool\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    x = x.reshape(x.shape[0],-1)\n",
        "    x = self.fc1(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "QSDhYO-MwODo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state,filename=\"my_checkpoint.pth.tar\"):\n",
        "  print(' => Saving Checkpoint')\n",
        "  torch.save(state,filename)"
      ],
      "metadata": {
        "id": "r38WB7hpxtm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(checkpoint):\n",
        "  print(' => Loading Checkpoint')\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])"
      ],
      "metadata": {
        "id": "MGIGbyqFylhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Set Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "p80iSWC8wSVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Hyperparameters\n",
        "in_channels=1\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "learning_rate = 1e-4\n",
        "batch_size = 1024\n",
        "num_epochs = 10\n",
        "load_model = True"
      ],
      "metadata": {
        "id": "WGo0wBZTwUkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load Data\n",
        "train_dataset = datasets.MNIST(root='dataset/',train=True,transform=transforms.ToTensor(),download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='dataset/',train=False,transform=transforms.ToTensor(),download=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "Bt-tn9XkwaaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize Network\n",
        "model = CNN(in_channels=in_channels,num_classes=num_classes).to(device)"
      ],
      "metadata": {
        "id": "dFHhrRPQwdru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "metadata": {
        "id": "WdpTypZuwgNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if load_model:\n",
        "  load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI28oihFzmyf",
        "outputId": "e28fabe9-e8c2-40fb-a4e2-66576d8551db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " => Loading Checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Train Network\n",
        "for epoch in range(num_epochs):\n",
        "  losses = []\n",
        "\n",
        "  if epoch % 3 == 0:\n",
        "    checkpoint = {'state_dict':model.state_dict(),'optimizer':optimizer.state_dict()}\n",
        "    save_checkpoint(checkpoint)\n",
        "\n",
        "  for batch_idx,(data,targets) in enumerate(tqdm(train_loader)):\n",
        "    # Get data to cuda\n",
        "    data = data.to(device=device)\n",
        "    targets = targets.to(device=device)\n",
        "\n",
        "    # Forward\n",
        "    scores = model(data)\n",
        "    loss = criterion(scores,targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient descent or Adam step size\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMVQjzxFwiDm",
        "outputId": "6784c26d-3805-48f6-ffd0-0832fe358fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " => Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 59/59 [00:04<00:00, 12.43it/s]\n",
            "100%|██████████| 59/59 [00:04<00:00, 12.47it/s]\n",
            "100%|██████████| 59/59 [00:04<00:00, 12.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " => Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 59/59 [00:04<00:00, 12.37it/s]\n",
            "100%|██████████| 59/59 [00:04<00:00, 12.64it/s]\n",
            "100%|██████████| 59/59 [00:04<00:00, 12.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " => Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 59/59 [00:04<00:00, 12.64it/s]\n",
            "100%|██████████| 59/59 [00:04<00:00, 12.67it/s]\n",
            "100%|██████████| 59/59 [00:04<00:00, 12.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " => Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 59/59 [00:04<00:00, 12.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Check Accuracy on training & test to see how good our model \n",
        "def check_accuracy(loader,model):\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x,y  in loader:\n",
        "      x = x.to(device=device)\n",
        "      y = y.to(device=device)\n",
        "\n",
        "      # Forward\n",
        "      scores = model(x)\n",
        "      #64x10\n",
        "      _,predictions = scores.max(1) #(max values for 2nd dim)/will output indices\n",
        "      \n",
        "      num_correct += (predictions == y).sum()\n",
        "      num_samples += predictions.size(0) #Size of 1st dimension\n",
        "\n",
        "  model.train()\n",
        "  return num_correct/num_samples\n",
        "\n",
        "print(f'Accuracy on training data: {check_accuracy(train_loader, model)*100:.2f}')\n",
        "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWj93iEnwi3G",
        "outputId": "943ce702-0065-40cf-dd99-6ac47120618c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training data: 91.60\n",
            "Accuracy on test set: 91.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5p5RyPvIy8Is"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}